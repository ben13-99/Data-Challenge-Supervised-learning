# -*- coding: utf-8 -*-
"""Data challenge de r√©gression - Notebook final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cLPF4qDNMFRK1D7I9N9fhVFEYnqKABNH
"""

from google.colab import files

# importer train_data.csv, test_data.csv, data.csv et dataset.csv
files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_train=pd.read_csv('train_data.csv')
df_test=pd.read_csv('test_data.csv')

"""# === Enrichissements externes ==="""

# @title [Audit anti-fuite] V√©rifier que `data` ne contient pas d'infos de `test_data`
import pandas as pd
import numpy as np

# Chargement (adapte les chemins si besoin)
df_train = pd.read_csv("train_data.csv")
df_test  = pd.read_csv("test_data.csv")
df_data  = pd.read_csv("data.csv")

# Colonnes communes pour construire un fingerprint (cat√©gorielles + continues arrondies)
common = list(set(df_train.columns) & set(df_test.columns) & set(df_data.columns))
cats   = [c for c in ["key","mode","explicit"] if c in common]
conts  = [c for c in ["tempo","duration_ms","danceability","energy","loudness"] if c in common]

def make_fingerprint(df, cats, conts):
    parts = []
    if cats:
        parts.append(df[cats].astype(str).agg("|".join, axis=1))
    for c in conts:
        x = pd.to_numeric(df[c], errors="coerce")
        if c == "duration_ms":
            parts.append((np.round(x/1000, 0)).astype("Int64").astype(str))   # secondes
        elif c == "tempo":
            parts.append((np.round(x/2, 0)*2).astype("Int64").astype(str))    # bpm pair le + proche
        elif c == "loudness":
            parts.append((np.round(x, 0)).astype("Int64").astype(str))        # dB arrondi √† l'unit√©
        else:
            parts.append((np.round(x, 2)).astype(str))                        # 2 d√©cimales
    if not parts:
        return pd.Series([""]*len(df))
    return pd.Series(["|".join(vals) for vals in zip(*parts)])

fp_test = make_fingerprint(df_test, cats, conts)
fp_data = make_fingerprint(df_data, cats, conts)
overlap = len(set(fp_test) & set(fp_data))

print("Colonnes utilis√©es pour le fingerprint:", cats + conts)
print("Taille test:", len(df_test), "| Taille data:", len(df_data))
print("‚ö†Ô∏è Overlap exact fingerprint(data ‚à© test):", overlap)

if overlap == 0:
    print("‚úÖ Aucun recouvrement d√©tect√© : `data` ne semble pas contenir de lignes de `test_data`.")
else:
    print("üö® Attention : recouvrement non nul ‚Äî revois les r√®gles d‚Äôenrichissement.")

# @title Enrichir df_train avec data.csv (anti-fuite, filtres de plausibilit√©, alignement colonnes)
import pandas as pd
import numpy as np
from typing import List

# === Chemins (adapte si besoin) ===
train_path = "train_data.csv"
test_path  = "test_data.csv"
data_path  = "data.csv"
out_path   = "train_augmented.csv"

# === 0) Chargement ===
df_train = pd.read_csv(train_path)
df_test  = pd.read_csv(test_path)
df_data  = pd.read_csv(data_path)

# Suppression des colonnes 'index-like' si pr√©sentes
for col in ["Unnamed: 0", "index"]:
    if col in df_train.columns: df_train.drop(columns=col, inplace=True)
    if col in df_test.columns:  df_test.drop(columns=col, inplace=True)
    if col in df_data.columns:  df_data.drop(columns=col, inplace=True)

# === 1) Colonnes cibles de ta pipeline (on s'aligne automatiquement sur le train) ===
# On prend comme r√©f√©rence les colonnes du train pour l'ordre final
train_cols = list(df_train.columns)

# Quelques colonnes ‚Äúcandidates‚Äù utiles si pr√©sentes dans data
maybe_cat = ["track_genre", "time_signature", "explicit", "mode", "key"]
maybe_num = ["danceability","energy","loudness","speechiness","acousticness",
             "instrumentalness","liveness","valence","tempo","duration_ms","popularity"]

# === 2) Filtres de plausibilit√© sur data (conservateurs) ===
def plausible(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    # bornes type Spotify
    rng01 = ['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence']
    for c in rng01:
        if c in d.columns:
            x = pd.to_numeric(d[c], errors='coerce')
            d = d[(x.isna()) | ((x >= 0) & (x <= 1))]
    if "tempo" in d.columns:
        x = pd.to_numeric(d["tempo"], errors='coerce')
        d = d[(x.isna()) | ((x > 0) & (x < 250))]
    if "duration_ms" in d.columns:
        x = pd.to_numeric(d["duration_ms"], errors='coerce')
        d = d[(x.isna()) | ((x > 5_000) & (x < 1.5e7))]  # 5s ‚Üí ~4h
    if "loudness" in d.columns:
        x = pd.to_numeric(d["loudness"], errors='coerce')
        d = d[(x.isna()) | ((x >= -60) & (x <= 5))]
    return d

df_data_pl = plausible(df_data)

# === 3) Pr√©parer des colonnes manquantes c√¥t√© data (sans casser ton pipeline) ===
# a) Ajoute placeholders pour cateÃÅgorielles manquantes
for c in maybe_cat:
    if (c not in df_data_pl.columns) and (c in train_cols):
        # placeholder ‚Äúsafe‚Äù : NaN sauf track_genre (‚Äòunknown‚Äô pour TE ‚Üí moyenne globale)
        df_data_pl[c] = "unknown" if c == "track_genre" else np.nan

# b) Convertit num√©riques possibles
for c in maybe_num:
    if c in df_data_pl.columns:
        df_data_pl[c] = pd.to_numeric(df_data_pl[c], errors="coerce")

# c) Garde uniquement colonnes utiles (toutes celles pr√©sentes dans le train + target si dans data)
keep_cols = [c for c in train_cols if c in df_data_pl.columns]
missing_in_data = [c for c in train_cols if c not in df_data_pl.columns]

# Ajoute les colonnes du train manquantes c√¥t√© data en NaN (pour pouvoir concat√©ner)
for c in missing_in_data:
    df_data_pl[c] = np.nan

# R√©ordonne comme le train
df_data_pl = df_data_pl[train_cols].copy()

# === 4) Fingerprint anti-duplication (√©viter recouvrement avec train ET test) ===
# On fabrique une ‚Äúempreinte‚Äù robuste √† petites variations pour identifier des doublons probables
def make_fingerprint(df: pd.DataFrame) -> pd.Series:
    parts: List[pd.Series] = []

    # Cat√©gories stables si existantes
    for c in ["key","mode","explicit"]:
        if c in df.columns:
            parts.append(df[c].astype(str))
    # Continues arrondies (stabilise la comparaison)
    def num_s(df, col, scale="raw"):
        x = pd.to_numeric(df[col], errors="coerce")
        if col == "duration_ms":
            return (x/1000).round().astype("Int64").astype(str)    # secondes
        if col == "tempo":
            return (np.round(x/2, 0)*2).astype("Int64").astype(str)# bpm pair
        if col == "loudness":
            return x.round().astype("Int64").astype(str)           # dB √† l'unit√©
        return x.round(2).astype(str)                              # autres √† 2 d√©cimales

    for c in ["tempo","duration_ms","danceability","energy","loudness"]:
        if c in df.columns:
            parts.append(num_s(df, c))

    if not parts:
        return pd.Series([""]*len(df), index=df.index)

    return pd.Series(["|".join(vals) for vals in zip(*parts)], index=df.index)

fp_train = make_fingerprint(df_train)
fp_test  = make_fingerprint(df_test)
fp_data  = make_fingerprint(df_data_pl)

train_set = set(fp_train.dropna())
test_set  = set(fp_test.dropna())

mask_new = (~fp_data.isin(train_set)) & (~fp_data.isin(test_set))
df_new_rows = df_data_pl[mask_new].copy()

# === 5) Concat√©ner proprement & sauvegarder ===
df_train_aug = pd.concat([df_train, df_new_rows], ignore_index=True)

# Sauvegarde
df_train_aug.to_csv(out_path, index=False)

# Remplace df_train en m√©moire (pour la suite du notebook)
df_train = df_train_aug

# === 6) Rapport succinct ===
print("‚úÖ Enrichissement termin√© (anti-fuite).")
print(f" - Lignes train AVANT : {len(fp_train):,}")
print(f" - Nouvelles lignes ajout√©es depuis data : {len(df_new_rows):,}")
print(f" - Lignes train APR√àS : {len(df_train):,}")
print(f" - Fichier √©crit : {out_path}")

# @title [Enrichissement #2] Ajouter les lignes exploitables de dataset.csv (anti-fuite + alignement)
import pandas as pd
import numpy as np
from typing import List

# Hypoth√®ses: df_train (d√©j√† enrichi avec data.csv) et df_test sont en m√©moire
# Fichiers:
ds_path = "dataset.csv"

# 0) Charger et nettoyer dataset
df_ds = pd.read_csv(ds_path)

# Drop colonnes index-like si pr√©sentes
for col in ["Unnamed: 0", "index"]:
    if col in df_ds.columns:
        df_ds.drop(columns=col, inplace=True)

# 1) Harmoniser les colonnes pour compatibilit√© concat
train_cols = list(df_train.columns)

# placeholders pour cateÃÅgorielles utiles
for c in ["track_genre","time_signature","explicit","mode","key"]:
    if (c not in df_ds.columns) and (c in train_cols):
        df_ds[c] = "unknown" if c == "track_genre" else np.nan

# coercition numeÃÅrique
for c in ["danceability","energy","loudness","speechiness","acousticness",
          "instrumentalness","liveness","valence","tempo","duration_ms","popularity"]:
    if c in df_ds.columns:
        df_ds[c] = pd.to_numeric(df_ds[c], errors="coerce")

# Filtres de plausibilit√© conservateurs
def plausible(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    rng01 = ['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence']
    for c in rng01:
        if c in d.columns:
            x = pd.to_numeric(d[c], errors='coerce')
            d = d[(x.isna()) | ((x >= 0) & (x <= 1))]
    if "tempo" in d.columns:
        x = pd.to_numeric(d["tempo"], errors='coerce')
        d = d[(x.isna()) | ((x > 0) & (x < 250))]
    if "duration_ms" in d.columns:
        x = pd.to_numeric(d["duration_ms"], errors='coerce')
        d = d[(x.isna()) | ((x > 5_000) & (x < 1.5e7))]
    if "loudness" in d.columns:
        x = pd.to_numeric(d["loudness"], errors='coerce')
        d = d[(x.isna()) | ((x >= -60) & (x <= 5))]
    return d

df_ds = plausible(df_ds)

# 2) Fingerprint anti-duplication (√©viter doublons avec train & test)
def make_fingerprint(df: pd.DataFrame) -> pd.Series:
    parts: List[pd.Series] = []
    for c in ["key","mode","explicit"]:
        if c in df.columns: parts.append(df[c].astype(str))
    def num_s(df, col):
        x = pd.to_numeric(df[col], errors="coerce")
        if col == "duration_ms": return (x/1000).round().astype("Int64").astype(str)
        if col == "tempo":       return (np.round(x/2,0)*2).astype("Int64").astype(str)
        if col == "loudness":    return x.round().astype("Int64").astype(str)
        return x.round(2).astype(str)
    for c in ["tempo","duration_ms","danceability","energy","loudness"]:
        if c in df.columns: parts.append(num_s(df, c))
    if not parts:
        return pd.Series([""]*len(df), index=df.index)
    return pd.Series(["|".join(vals) for vals in zip(*parts)], index=df.index)

fp_train = make_fingerprint(df_train)
fp_test  = make_fingerprint(df_test)
fp_ds    = make_fingerprint(df_ds)

mask_new = (~fp_ds.isin(set(fp_train.dropna()))) & (~fp_ds.isin(set(fp_test.dropna())))
df_ds_new = df_ds[mask_new].copy()

# 3) Aligner l'ordre des colonnes pour concat
missing_in_ds = [c for c in train_cols if c not in df_ds_new.columns]
for c in missing_in_ds:
    df_ds_new[c] = np.nan
df_ds_new = df_ds_new[train_cols].copy()

# 4) Concat ‚Üí df_train enrichi (2·µâ vague)
df_train = pd.concat([df_train, df_ds_new], ignore_index=True)

print("‚úÖ Enrichissement #2 dataset.csv termin√©.")
print(f" - Nouvelles lignes ajout√©es depuis dataset.csv : {len(df_ds_new):,}")
print(f" - Nouvelles dimensions df_train : {df_train.shape}")

# @title [Features externes] Priors par ARTISTE / ALBUM / GENRE issus de dataset.csv (+ utilisables pour train & test)
import pandas as pd
import numpy as np

# Rechargement rapide de dataset pour calculer les priors (au cas o√π il n'est plus en m√©moire)
df_ds = pd.read_csv("dataset.csv")
for col in ["Unnamed: 0","index"]:
    if col in df_ds.columns:
        df_ds.drop(columns=col, inplace=True)

# Cast num√©riques
for c in ["popularity","duration_ms","tempo","loudness"]:
    if c in df_ds.columns:
        df_ds[c] = pd.to_numeric(df_ds[c], errors="coerce")

GLOBAL_MEAN = float(pd.to_numeric(df_ds["popularity"], errors="coerce").mean())
def smooth_mean(cnt, mu, global_mu, m):
    return (cnt*mu + m*global_mu) / (cnt + m)

# 1) Priors par ARTISTE (si pr√©sent)
pri_artist = None
if "artists" in df_ds.columns:
    g = (df_ds
         .groupby("artists")["popularity"]
         .agg(['count','mean'])
         .rename(columns={'count':'cnt','mean':'mu'})
         .reset_index())
    g["ext_prior_artist_pop"] = smooth_mean(g["cnt"], g["mu"], GLOBAL_MEAN, m=200)
    g["artist_track_count"]   = g["cnt"]
    pri_artist = g[["artists","ext_prior_artist_pop","artist_track_count"]]

# 2) Priors par ALBUM (si pr√©sent)
pri_album = None
if "album_name" in df_ds.columns:
    g = (df_ds
         .groupby("album_name")["popularity"]
         .agg(['count','mean'])
         .rename(columns={'count':'cnt','mean':'mu'})
         .reset_index())
    g["ext_prior_album_pop"] = smooth_mean(g["cnt"], g["mu"], GLOBAL_MEAN, m=150)
    g["album_track_count"]   = g["cnt"]
    pri_album = g[["album_name","ext_prior_album_pop","album_track_count"]]

# 3) Prior par GENRE (utile pour anciennes lignes ne disposant pas d'artiste/album)
pri_genre = None
if "track_genre" in df_ds.columns:
    g = (df_ds
         .groupby("track_genre")["popularity"]
         .agg(['count','mean'])
         .rename(columns={'count':'cnt','mean':'mu'})
         .reset_index())
    g["ext_prior_genre_pop_ds"] = smooth_mean(g["cnt"], g["mu"], GLOBAL_MEAN, m=300)
    pri_genre = g[["track_genre","ext_prior_genre_pop_ds"]]

# 4) Merge de ces priors dans df_train et df_test (l√† o√π c'est possible)
def enrich_with_priors(df):
    out = df.copy()
    if (pri_artist is not None) and ("artists" in out.columns):
        out = out.merge(pri_artist, on="artists", how="left")
        # Indicateur simple: collaboration (plusieurs artistes)
        out["is_collab"] = out["artists"].astype(str).str.contains(r",|&|;| feat\. ", case=False, regex=True).astype(float)
    else:
        out["ext_prior_artist_pop"] = np.nan
        out["artist_track_count"]   = np.nan
        out["is_collab"]            = np.nan

    if (pri_album is not None) and ("album_name" in out.columns):
        out = out.merge(pri_album, on="album_name", how="left")
    else:
        out["ext_prior_album_pop"] = np.nan
        out["album_track_count"]   = np.nan

    if (pri_genre is not None) and ("track_genre" in out.columns):
        out = out.merge(pri_genre, on="track_genre", how="left")
    else:
        out["ext_prior_genre_pop_ds"] = np.nan

    # Colonne backoff g√©n√©rale (utilisable partout): artiste ‚Üí album ‚Üí genre ‚Üí moyenne globale
    prior_cols = ["ext_prior_artist_pop","ext_prior_album_pop","ext_prior_genre_pop_ds"]
    out["ext_prior_pop_ds"] = np.nan
    for c in prior_cols:
        if c in out.columns:
            out["ext_prior_pop_ds"] = out["ext_prior_pop_ds"].fillna(out[c])
    out["ext_prior_pop_ds"] = out["ext_prior_pop_ds"].fillna(GLOBAL_MEAN)

    # Remplir les compteurs manquants par 0 (utile pour arbres)
    for c in ["artist_track_count","album_track_count"]:
        if c in out.columns:
            out[c] = out[c].fillna(0)

    return out

df_train = enrich_with_priors(df_train)
df_test  = enrich_with_priors(df_test)

print("‚úÖ Priors externes ajout√©s.")
cols_added = [c for c in ["ext_prior_artist_pop","artist_track_count","is_collab",
                          "ext_prior_album_pop","album_track_count",
                          "ext_prior_genre_pop_ds","ext_prior_pop_ds"] if c in df_train.columns]
print("Colonnes ajout√©es:", cols_added)
print("df_train:", df_train.shape, " | df_test:", df_test.shape)

# >> A ce stade:
# - df_train contient nouvelles lignes (de dataset.csv) + priors (artiste/album/genre) + backoff 'ext_prior_pop_ds'
# - df_test contient priors genre (pour toutes lignes) et artiste/album si jamais ces colonnes existent apr√®s enrichissements pr√©c√©dents

"""# I - Corr√©lation des variables

## Analyse des donn√©es
"""

df_train.info()
df_test.info()

df_train.head()

df_test.head()

var_continue=df_train.select_dtypes(include='float64').columns.tolist()+['duration_ms']
df_continue=df_train[var_continue]
df_continue.head()

var_categorielle=['explicit','key','mode','time_signature','track_genre']
df_cat=df_train[var_categorielle]
df_cat.head()

"""## 1) Corr√©lations variable continue <-> variable continue

### Visualisation si les hypoth√®ses sont v√©rifi√©es (lin√©arit√©, normalit√©, dispertion et outliers)
"""

# --- Cellule 1 : imports (Colab) ---
!pip -q install statsmodels

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from statsmodels.api import qqplot
from statsmodels.nonparametric.smoothers_lowess import lowess
from statsmodels.stats.diagnostic import het_breuschpagan
import statsmodels.api as sm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from statsmodels.api import qqplot
from statsmodels.nonparametric.smoothers_lowess import lowess

def pearson_diagnostics_plots(df, x, y, sample=None):
    """
    Version compacte pour Colab :
    - Figures 2√ó plus petites (~6√ó7)
    - Police et l√©gendes adapt√©es
    - Affichage fluide pour 4 graphiques c√¥te √† c√¥te
    """
    d = df[[x, y]].dropna()
    if sample and len(d) > sample:
        d = d.sample(sample, random_state=42)

    X = d[[x]].values
    yv = d[y].values
    lr = LinearRegression().fit(X, yv)
    fitted = lr.predict(X)
    resid = yv - fitted
    resid_std = (resid - resid.mean()) / resid.std(ddof=1)

    # Figure plus petite (moiti√© de la taille pr√©c√©dente)
    fig, axes = plt.subplots(3, 2, figsize=(6.5, 7))
    axes = axes.ravel()

    title_style = dict(fontsize=9, weight='bold')
    label_style = dict(fontsize=8)

    # 1Ô∏è‚É£ Lin√©arit√©
    sns.scatterplot(x=d[x], y=d[y], ax=axes[0], s=10, alpha=0.6)
    xs = np.linspace(d[x].min(), d[x].max(), 200).reshape(-1, 1)
    axes[0].plot(xs, lr.predict(xs), color="red", linewidth=1.2, label="R√©gression lin√©aire")
    lo = lowess(yv, d[x].values, frac=0.3)
    axes[0].plot(lo[:, 0], lo[:, 1], color="orange", linestyle="--", linewidth=1.2, label="LOWESS")
    axes[0].set_title(f"1Ô∏è‚É£ Lin√©arit√© : {x} vs {y}", **title_style)
    axes[0].set_xlabel(x, **label_style)
    axes[0].set_ylabel(y, **label_style)
    axes[0].legend(fontsize=7)

    # 2Ô∏è‚É£ Homosc√©dasticit√©
    sns.scatterplot(x=fitted, y=resid, ax=axes[1], s=10, alpha=0.6)
    axes[1].axhline(0, color='red', linestyle='--', linewidth=0.8)
    axes[1].set_title(f"2Ô∏è‚É£ R√©sidus vs ajust√©s ({x}‚Üí{y})", **title_style)
    axes[1].set_xlabel(f"Ajust√©s {y}", **label_style)
    axes[1].set_ylabel("R√©sidus", **label_style)

    # 3Ô∏è‚É£ Normalit√©
    qqplot(resid, line="s", ax=axes[2])
    axes[2].set_title(f"3Ô∏è‚É£ QQ-plot r√©sidus ({x}, {y})", **title_style)
    axes[2].set_xlabel("Quantiles th√©oriques", **label_style)
    axes[2].set_ylabel("Quantiles observ√©s", **label_style)

    # 4Ô∏è‚É£ Scale‚ÄìLocation
    sns.scatterplot(x=fitted, y=np.abs(resid_std), ax=axes[3], s=10, alpha=0.6)
    axes[3].set_title(f"4Ô∏è‚É£ Variance constante : {x}‚Üí{y}", **title_style)
    axes[3].set_xlabel(f"Ajust√©s {y}", **label_style)
    axes[3].set_ylabel("|R√©sidus standardis√©s|", **label_style)

    # 5Ô∏è‚É£ Outliers sur X
    sns.boxplot(y=d[x], ax=axes[4], color="skyblue")
    axes[4].set_title(f"5Ô∏è‚É£ Outliers : {x}", **title_style)
    axes[4].set_ylabel(x, **label_style)

    # 6Ô∏è‚É£ Outliers sur Y
    sns.boxplot(y=d[y], ax=axes[5], color="lightgreen")
    axes[5].set_title(f"6Ô∏è‚É£ Outliers : {y}", **title_style)
    axes[5].set_ylabel(y, **label_style)

    plt.tight_layout(pad=1.2)
    plt.show()

# Exemple d‚Äôusage :
for var_cont1 in var_continue:
  for var_cont2 in var_continue:
    if var_cont1 != var_cont2:
      pearson_diagnostics_plots(df_continue, x=var_cont1, y=var_cont2, sample=5000)

"""### S√©lections des variables, couples de variables v√©rifiant les hypoth√®ses"""

# @title Lin√©arit√©
from itertools import combinations
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

def couples_lineaires(df, min_r2=0.30, max_delta_quad=0.05, cols_num=None):
    if cols_num is None:
        cols_num = df.select_dtypes(include=np.number).columns.tolist()
    pairs = []
    for x, y in combinations(cols_num, 2):
        d = df[[x, y]].dropna()
        if len(d) < 10:
            continue
        X = d[[x]].values; yv = d[y].values
        # Lin√©aire
        lr = LinearRegression().fit(X, yv)
        r2_lin = r2_score(yv, lr.predict(X))
        # Quadratique (pour d√©tecter une non-lin√©arit√© √©vidente)
        PF = PolynomialFeatures(degree=2, include_bias=False)
        Xq = PF.fit_transform(X)
        lrq = LinearRegression().fit(Xq, yv)
        r2_quad = r2_score(yv, lrq.predict(Xq))
        if (r2_lin >= min_r2) and ((r2_quad - r2_lin) < max_delta_quad):
            pairs.append((x, y))
    return pairs

# Exemple :
couples_lineaires(df_continue, min_r2=0.25, max_delta_quad=0.07)

# @title Normalit√©
from scipy.stats import shapiro, normaltest

def couples_normaux(df, alpha=0.05, cols_num=None, max_shapiro_n=5000):
    if cols_num is None:
        cols_num = df.select_dtypes(include=np.number).columns.tolist()
    def p_normal(v):
        v = pd.Series(v).dropna().values
        if len(v) < 8:
            return 0.0
        if len(v) <= max_shapiro_n:
            return shapiro(v)[1]
        else:
            return normaltest(v)[1]
    pairs = []
    for x, y in combinations(cols_num, 2):
        px = p_normal(df[x]); py = p_normal(df[y])
        if (px > alpha) and (py > alpha):
            pairs.append((x, y))
    return pairs

# Exemple :
couples_normaux(df_continue, alpha=0.05)

# @title Dispertion (Homosc√©dasticit√©)
def couples_homoscedastiques(df, alpha=0.05, cols_num=None):
    if cols_num is None:
        cols_num = df.select_dtypes(include=np.number).columns.tolist()
    pairs = []
    for x, y in combinations(cols_num, 2):
        d = df[[x, y]].dropna()
        if len(d) < 10:
            continue
        X = sm.add_constant(d[[x]])
        model = sm.OLS(d[y], X).fit()
        # Breusch‚ÄìPagan
        bp_test = het_breuschpagan(model.resid, model.model.exog)
        pval = bp_test[3]  # p-value de la statistique F
        if pval > alpha:
            pairs.append((x, y))
    return pairs

# Exemple :
couples_homoscedastiques(df_continue, alpha=0.05)

# @title Outliers
def couples_sans_outliers(df, max_frac_outliers=0.01, z_thresh=3.5, cols_num=None):
    if cols_num is None:
        cols_num = df.select_dtypes(include=np.number).columns.tolist()
    pairs = []
    for x, y in combinations(cols_num, 2):
        d = df[[x, y]].dropna()
        if len(d) < 10:
            continue
        zx = np.abs((d[x] - d[x].mean()) / d[x].std(ddof=1))
        zy = np.abs((d[y] - d[y].mean()) / d[y].std(ddof=1))
        frac = ((zx > z_thresh) | (zy > z_thresh)).mean()
        if frac <= max_frac_outliers:
            pairs.append((x, y))
    return pairs

# Exemple :
couples_sans_outliers(df_continue, max_frac_outliers=0.02, z_thresh=3.5)

"""### Tests de Pearson et de Spearman

#### Pearson pour les couples avec une relation lin√©aire
"""

df_pearson1=df_continue[['acousticness','energy']]
df_pearson1.corr(method='pearson')

df_pearson2=df_continue[['acousticness','loudness']]
df_pearson2.corr(method='spearman')

"""#### Spearman pour tout les couples"""

corr_continue_spear=df_continue.corr(method='spearman')
print(corr_continue_spear)
sns.heatmap(corr_continue_spear)
plt.show()

"""## 2) Corr√©lations variable continue <-> variable cat√©gorielle

### Test de normalit√© et de variances pour les distributions groupes par groupes pour chaque couple variable cat√©gorielle <-> variable continue
"""

from scipy.stats import shapiro, levene

# Normalit√© (sur chaque groupe) si p<0.05 la distribution n'est probablement pas normale
for var_cont in var_continue:
  for var_cat in var_categorielle:
    for name, group in df_train.groupby(var_cat):
      stat, p = shapiro(group[f"{var_cont}"])
      # name correspond au nom du groupe par exemple dans track_genre -> acoustic
      print(f"{var_cont}-{var_cat}",f"group:{name}", "‚Üí p =", round(p, 4))

import pandas as pd
import numpy as np
from scipy.stats import shapiro

def pairs_normal_by_group(
    df: pd.DataFrame,
    num_cols,
    cat_cols,
    alpha: float = 0.05,
    min_group_n: int = 5,      # n min par modalit√© pour tester
    max_n_shapiro: int = 5000, # si n trop grand, on √©chantillonne pour Shapiro
    random_state: int = 0
):
    """
    Retourne une liste tri√©e de tuples (var_cont, var_cat) pour lesquels,
    dans chaque modalit√© du facteur (avec n >= min_group_n), le test de Shapiro
    ne rejette pas la normalit√© (p > alpha).
    """
    if isinstance(num_cols, str): num_cols = [num_cols]
    if isinstance(cat_cols, str): cat_cols = [cat_cols]

    ok_pairs = set()
    for c in cat_cols:
        gdf = df.dropna(subset=[c])
        for x in num_cols:
            all_ok = True
            tested_levels = 0
            for level, g in gdf.groupby(c, dropna=False):
                vals = pd.to_numeric(g[x], errors="coerce").dropna()
                n = len(vals)
                if n >= min_group_n:
                    tested_levels += 1
                    v = vals
                    if n > max_n_shapiro:
                        v = vals.sample(max_n_shapiro, random_state=random_state)
                    p = shapiro(v).pvalue
                    if not (p > alpha):
                        all_ok = False
                        break
            if tested_levels >= 2 and all_ok:
                ok_pairs.add((x, c))

    return sorted(ok_pairs)

# --- EXEMPLE D‚ÄôAPPEL ---
# num_cols = ['revenu', 'age', 'score']
# cat_cols = ['niveau_etude', 'region']
# normal_ok = pairs_normal_by_group(df_train, num_cols, cat_cols, alpha=0.05, min_group_n=5)
# print(normal_ok)  # -> liste de tuples (var_cont, var_cat)

pairs_normal_by_group(df_train,num_cols=var_continue,cat_cols=var_categorielle,alpha=0.05, min_group_n=5)

"""Les variables continues ne suivent pas une loi normale par rapport aux variables cat√©gorielles"""

import pandas as pd
import numpy as np
from scipy.stats import levene

def pairs_homoscedastic(
    df: pd.DataFrame,
    num_cols,
    cat_cols,
    alpha: float = 0.05,
    min_group_n: int = 5,      # n min par modalit√© pour inclure dans Levene
    center: str = "median"     # "mean" (Levene) ou "median" (Brown‚ÄìForsythe, +robuste)
):
    """
    Retourne une liste tri√©e de tuples (var_cont, var_cat) qui passent l‚Äôhomosc√©dasticit√©
    (test de Levene/Brown‚ÄìForsythe) avec p > alpha, en ne gardant que les modalit√©s
    ayant n >= min_group_n (au moins 2 modalit√©s requises).
    """
    if isinstance(num_cols, str): num_cols = [num_cols]
    if isinstance(cat_cols, str): cat_cols = [cat_cols]

    ok_pairs = set()
    for c in cat_cols:
        gdf = df.dropna(subset=[c])
        for x in num_cols:
            groups = []
            for _, g in gdf.groupby(c, dropna=False):
                vals = pd.to_numeric(g[x], errors="coerce").dropna()
                if len(vals) >= min_group_n:
                    groups.append(vals.values)
            if len(groups) >= 2 and sum(len(g) > 1 for g in groups) >= 2:
                p = levene(*groups, center=center).pvalue
                if p > alpha:
                    ok_pairs.add((x, c))
    return sorted(ok_pairs)

# --- EXEMPLE D‚ÄôAPPEL ---
# homo_ok = pairs_homoscedastic(df_train, num_cols, cat_cols, alpha=0.05, min_group_n=5, center="median")
# print(homo_ok)  # -> liste de tuples (var_cont, var_cat)

pairs_homoscedastic(df_train, var_continue, var_categorielle, alpha=0.05, min_group_n=5, center="median")

"""Encore une fois les variables ont des variances trop diff√©rentes entre plusieurs groupes diff√©rent d'une variable cat√©gorielle

### Test de Kruskal‚ÄìWallis car plus pertinent que l'anova qui suppose les donn√©es suivant une loi normale et de variances similaire
"""

from scipy.stats import kruskal
import numpy as np

for var_cont in var_continue:
    for var_cat in var_categorielle:
        # on garde uniquement les colonnes utiles et on enl√®ve les NaN
        tmp = df_train[[var_cont, var_cat]].dropna()

        # on construit la liste des groupes (au moins 2 obs par groupe)
        groups = [sub[var_cont].values
                  for _, sub in tmp.groupby(var_cat) if len(sub) >= 2]

        # il faut au moins 2 groupes pour Kruskal
        if len(groups) < 2:
            print(f"{var_cont} ~ {var_cat} : test non effectu√© (moins de 2 groupes valides).")
            continue

        h_stat, p_value = kruskal(*groups)
        print(f"{var_cont} ~ {var_cat} | k={len(groups)} | H={h_stat:.3f} | p={p_value:.4g}")

"""## 3) Corr√©lation variable cat√©gorielle <-> variable cat√©gorielle"""

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Fonction pour calculer le Cram√©r‚Äôs V
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]
    n = confusion_matrix.sum().sum()
    r, k = confusion_matrix.shape
    return np.sqrt(chi2 / (n * (min(k, r) - 1)))

# Exemple : df_cat contient uniquement les variables cat√©gorielles
# var_categorielle = ['explicit', 'key', 'mode', 'time_signature', 'track_genre']

for var_cat1 in var_categorielle:
    for var_cat2 in var_categorielle:
        if var_cat1 != var_cat2:
            value = cramers_v(df_cat[var_cat1], df_cat[var_cat2])
            print(f"{var_cat1} vs {var_cat2} ‚Üí Cram√©r‚Äôs V = {value:.3f}")

"""# II - Traitement des donn√©es en fonction des r√©sultats des tests statistique

## 1) Encodage des donn√©es
"""

# @title Encoding track_genre
import pandas as pd

def target_encode_smooth(train_df, test_df, cat_col, y_col='popularity', m=200):
    """
    Encodage par moyenne liss√©e :
    TE(cat) = (count_cat * mean_cat + m * global_mean) / (count_cat + m)
    """
    global_mean = train_df[y_col].mean()
    stats = (train_df.groupby(cat_col)[y_col]
             .agg(['mean','count'])
             .rename(columns={'mean':'mean_cat','count':'count_cat'}))
    stats['te'] = (stats['count_cat'] * stats['mean_cat'] + m * global_mean) / (stats['count_cat'] + m)
    train_df[cat_col + '_te'] = train_df[cat_col].map(stats['te']).fillna(global_mean)
    test_df[cat_col + '_te']  = test_df[cat_col].map(stats['te']).fillna(global_mean)
    return train_df, test_df

# applique sur tes dataframes
df_train, df_test = target_encode_smooth(df_train, df_test, 'track_genre', y_col='popularity', m=200)

# @title Diagnostic target-encoding: popularit√© moyenne par quantile de track_genre_te
import numpy as np, pandas as pd, matplotlib.pyplot as plt

assert 'track_genre_te' in df_train.columns
q = pd.qcut(df_train['track_genre_te'], q=10, duplicates='drop')
means = df_train.groupby(q)['popularity'].mean()
errs  = df_train.groupby(q)['popularity'].std()/np.sqrt(df_train.groupby(q)['popularity'].size())

plt.figure(figsize=(8,4))
plt.bar(range(len(means)), means.values, yerr=errs.values, capsize=3)
plt.xticks(range(len(means)), [f"Q{i+1}" for i in range(len(means))])
plt.ylabel("Popularit√© moyenne")
plt.title("Popularit√© vs d√©ciles de track_genre_te (fold-wise)")
plt.tight_layout(); plt.show()

"""## 2) Identifications des distributions et tendances"""

# @title === SCAN DISTRIBUTIONS & RECO ===
import numpy as np, pandas as pd
from scipy import stats
from sklearn.preprocessing import PowerTransformer, QuantileTransformer, FunctionTransformer

# Reprend tes listes (V3)
cont_cols = [
    'danceability','energy','loudness','speechiness','acousticness',
    'instrumentalness','liveness','valence','tempo','duration_ms','track_genre_te'
]

def suggest_transform(series: pd.Series):
    """Retourne une reco parmi: none, log1p, winsor, yeo-johnson, quantile."""
    x = pd.to_numeric(series, errors='coerce').dropna().values
    n = len(x)
    if n < 100:  # trop petit pour juger finement
        return "none"
    skew = stats.skew(x)
    kurt = stats.kurtosis(x)
    # test normalit√© (K¬≤) quand c'est raisonnable
    try:
        k2, p = stats.normaltest(x) if n <= 200_000 else (np.nan, np.nan)
    except Exception:
        k2, p = (np.nan, np.nan)

    # r√®gles simples et robustes
    # (tu peux les resserrer si tu veux √™tre plus/moins agressif)
    if np.nan_to_num(p) > 0.05 and abs(skew) < 0.75 and abs(kurt) < 3.5:
        return "none"
    if (series.min() >= 0) and (skew > 1.0):
        return "log1p"          # typique: duration_ms, speechiness, liveness
    if abs(skew) > 1.5 or abs(kurt) > 7:
        return "winsor"         # trop de queues -> clipping quantiles
    if abs(skew) > 0.75:
        return "yeo-johnson"    # donne souvent mieux que log si valeurs n√©gatives possibles
    return "quantile"           # file de secours pour distributions tr√®s bizarres

def scan_distributions(df, cols):
    rows = []
    for c in cols:
        s = pd.to_numeric(df[c], errors='coerce')
        s = s[~s.isna()]
        if len(s) == 0:
            rows.append((c, np.nan, np.nan, np.nan, "none"))
            continue
        skew = stats.skew(s.values)
        kurt = stats.kurtosis(s.values)
        try:
            k2, p = stats.normaltest(s.values if len(s) <= 200_000 else s.sample(200_000, random_state=0).values)
        except Exception:
            k2, p = (np.nan, np.nan)
        rec = suggest_transform(s)
        rows.append((c, skew, kurt, p, rec))
    return pd.DataFrame(rows, columns=["feature","skew","kurtosis","p_normaltest","recommendation"])

scan_df = scan_distributions(df_train, cont_cols).sort_values("recommendation")
scan_df

# @title Preprocesseur V3+
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PowerTransformer, QuantileTransformer, FunctionTransformer
import numpy as np
import pandas as pd


# Recommandations -> listes de colonnes par type de transform
rec_map = dict(zip(scan_df["feature"], scan_df["recommendation"]))
cols_log1p      = [c for c,r in rec_map.items() if r == "log1p"]
cols_winsor     = [c for c,r in rec_map.items() if r == "winsor"]
cols_yeojohnson = [c for c,r in rec_map.items() if r == "yeo-johnson"]
cols_quantile   = [c for c,r in rec_map.items() if r == "quantile"]
cols_none       = [c for c,r in rec_map.items() if r == "none"]

def winsorize_df(X, q_low=0.01, q_high=0.99):
    X = X.copy()
    for c in X.columns:
        lo, hi = np.quantile(X[c].values, [q_low, q_high])
        X[c] = np.clip(X[c].values, lo, hi)
    return X

winsor_tf = FunctionTransformer(lambda X: winsorize_df(pd.DataFrame(X, columns=cols_winsor)).values, feature_names_out="one-to-one")
log1p_tf  = FunctionTransformer(lambda X: np.log1p(np.clip(X, a_min=-0.999999, a_max=None)))
# Yeo-Johnson & Quantile
yeoj_tf   = PowerTransformer(method="yeo-johnson", standardize=False)
quant_tf  = QuantileTransformer(output_distribution="normal", n_quantiles=1000, subsample=200000, random_state=0)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

bin_cols      = ['explicit','mode']
low_card_cols = ['key','time_signature']
# Added 'track_genre_te' to cont_cols
cont_cols = list(rec_map.keys())

cont_blocks = []
if cols_none:
    cont_blocks.append(("cont_none", Pipeline([("imp", SimpleImputer(strategy="median"))]), cols_none))
if cols_log1p:
    cont_blocks.append(("cont_log1p", Pipeline([("imp", SimpleImputer(strategy="median")),("log", log1p_tf)]), cols_log1p))
if cols_winsor:
    cont_blocks.append(("cont_winsor", Pipeline([("imp", SimpleImputer(strategy="median")),("win", winsor_tf)]), cols_winsor))
if cols_yeojohnson:
    cont_blocks.append(("cont_yeoj", Pipeline([("imp", SimpleImputer(strategy="median")),("yeoj", yeoj_tf)]), cols_yeojohnson))
if cols_quantile:
    cont_blocks.append(("cont_quant", Pipeline([("imp", SimpleImputer(strategy="median")),("qt", quant_tf)]), cols_quantile))

cont_all = cols_none + cols_log1p + cols_winsor + cols_yeojohnson + cols_quantile
assert set(cont_all) == set(cont_cols), "Incoh√©rence colonnes continues"

bin_pipe = Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                     ("ohe", OneHotEncoder(drop='if_binary', handle_unknown='ignore'))])
low_pipe = Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                     ("ohe", OneHotEncoder(handle_unknown='ignore'))])

preprocessor_v3_plus = ColumnTransformer(cont_blocks + [
    ('bin',  bin_pipe,  bin_cols),
    ('low',  low_pipe,  low_card_cols),
], sparse_threshold=1.0)

print("Pr√©processeur V3+ pr√™t (transfos data-driven).")

"""# III - Entrainement du mod√®le final"""

# @title D√©coupage des donn√©es
from sklearn.model_selection import train_test_split

# === Donn√©es ===
X = df_train.drop(columns=['popularity'])
y = df_train['popularity']

# D√©coupage train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# @title MoE clusters (GMM) ‚Äî version rapide sans CV (responsabilit√©s + experts pond√©r√©s)
import numpy as np, pandas as pd, os
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

seed = 0
K_CLUST = 4
prepro = preprocessor_v3_plus
xgb_params = dict(
    learning_rate=0.032, max_depth=12, n_estimators=3000,
    subsample=0.8, colsample_bytree=0.65,
    min_child_weight=1, reg_lambda=0.9, reg_alpha=0.05,
    tree_method='hist', random_state=seed, n_jobs=-1
)

# Espace latent robuste (pas de PCA)
latent_cols = [c for c in ["energy","loudness","acousticness","danceability","speechiness","valence","tempo","duration_ms","year_norm"] if c in df_train.columns]
sc = StandardScaler()
Z_tr = sc.fit_transform(df_train[latent_cols].fillna(df_train[latent_cols].median()))
Z_te = sc.transform(df_test[latent_cols].fillna(df_train[latent_cols].median()))

# Router GMM: fit full train, responsabilit√©s = proba clusters
gmm = GaussianMixture(n_components=K_CLUST, covariance_type="full", random_state=seed)
gmm.fit(Z_tr)
Q_tr = gmm.predict_proba(Z_tr)  # (n,K)
Q_te = gmm.predict_proba(Z_te)  # (m,K)

# Experts XGB avec sample_weight = responsabilit√©s
y_tr = df_train["popularity"].values
experts = []
for k in range(K_CLUST):
    pipe = Pipeline([
        ("preprocess", prepro),
        ("model", XGBRegressor(**xgb_params))
    ])
    sw = Q_tr[:, k]
    pipe.fit(df_train.drop(columns=["popularity"], errors="ignore"), y_tr,
             model__sample_weight=sw)
    experts.append(pipe)

# Pr√©dictions pond√©r√©es
preds_tr = np.column_stack([exp.predict(df_train.drop(columns=["popularity"], errors="ignore")) for exp in experts])
preds_te = np.column_stack([exp.predict(df_test.drop(columns=[], errors="ignore")) for exp in experts])
y_hat_tr = np.clip((Q_tr * preds_tr).sum(axis=1), 0, 100)
y_hat_te = np.clip((Q_te * preds_te).sum(axis=1), 0, 100)

# R¬≤ local proxy (router sur X_test)
try:
    Z_hold = sc.transform(X_test[latent_cols].fillna(df_train[latent_cols].median()))
    Q_hold = gmm.predict_proba(Z_hold)
    preds_hold = np.column_stack([exp.predict(X_test) for exp in experts])
    y_hold = np.clip((Q_hold * preds_hold).sum(axis=1), 0, 100)
    print(f"üß™ R¬≤ local (MoE clusters, no-CV) = {r2_score(y_test, y_hold):.4f}")
except Exception as e:
    print("(skip) R¬≤ local proxy non calcul√© :", e)

# Soumission
os.makedirs("submissions", exist_ok=True)
pd.DataFrame({"id": np.arange(len(y_hat_te)), "popularity": y_hat_te}).to_csv(
    "submissions/submission_moe_clusters_fast.csv", index=False
)
print("‚úÖ Soumission √©crite: submissions/submission_moe_clusters_fast.csv")

# @title Helper: √©crire une soumission Kaggle valide (row_id align√©)
import numpy as np, pandas as pd, os

def write_submission_kaggle(df_test, y_pred, path):
    sub = pd.DataFrame({
        "row_id": (df_test["row_id"].astype(int) if "row_id" in df_test.columns
                   else np.arange(len(df_test), dtype=int)),
        "popularity": np.clip(np.asarray(y_pred, dtype=float), 0, 100)
    })
    # Checks utiles
    assert len(sub) == len(df_test), "Taille pr√©dictions ‚â† taille test"
    assert sub["row_id"].is_unique, "row_id dupliqu√©s !"
    # tri optionnel (si Kaggle exige tri exact)
    sub = sub.sort_values("row_id").reset_index(drop=True)
    os.makedirs("submissions", exist_ok=True)
    sub.to_csv(path, index=False)
    print(f"‚úÖ Fichier √©crit: {path} | shape={sub.shape} | row_id min/max =",
          int(sub.row_id.min()), int(sub.row_id.max()))

write_submission_kaggle(df_test, y_hat_te, "submissions/submission_moe_clusters_fast.csv")

"""# Annexe - Historiques des mod√®les entrain√©es"""

# @title SVM (SVR) ‚Äî grille compacte + R¬≤(test)
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "C":       [0.5, 1.0, 2.0],
    "epsilon": [0.1, 0.2],
    "kernel":  ["rbf"],   # 'rbf' pour non-lin√©aire
    "gamma":   ["scale"], # gamma auto
}
print("Nombre de combinaisons test√©es:", len(grid["C"])*len(grid["epsilon"])*len(grid["kernel"])*len(grid["gamma"]))

best = (-1e9, None, None)
for C in grid["C"]:
    for eps in grid["epsilon"]:
        for ker in grid["kernel"]:
            for gm in grid["gamma"]:
                params = dict(C=C, epsilon=eps, kernel=ker, gamma=gm)
                pipe = Pipeline([
                    ("preprocess", preprocessor_v3_plus),
                    ("model", SVR(**params))
                ])
                pipe.fit(X_train, y_train)
                preds = np.clip(pipe.predict(X_test), 0, 100)
                r2 = r2_score(y_test, preds)
                print(f"SVR C={C}, eps={eps}, kernel={ker} -> R¬≤={r2:.4f}")
                if r2 > best[0]:
                    best = (r2, params, pipe)

print("\n[SVR] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

# @title AdaBoostRegressor ‚Äî grille l√©g√®re + R¬≤(test)
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "n_estimators":   [300, 600],
    "learning_rate":  [0.05, 0.10],
    "max_depth_base": [3, 5],
}
print("Nombre de combinaisons test√©es:", len(grid["n_estimators"])*len(grid["learning_rate"])*len(grid["max_depth_base"]))

best = (-1e9, None, None)
for ne in grid["n_estimators"]:
    for lr in grid["learning_rate"]:
        for md in grid["max_depth_base"]:
            base = DecisionTreeRegressor(max_depth=md, random_state=0)
            model = AdaBoostRegressor(
                estimator=base, n_estimators=ne, learning_rate=lr, random_state=0
            )
            pipe = Pipeline([
                ("preprocess", preprocessor_v3_plus),
                ("model", model)
            ])
            pipe.fit(X_train, y_train)
            preds = np.clip(pipe.predict(X_test), 0, 100)
            r2 = r2_score(y_test, preds)
            print(f"AdaBoost n={ne}, lr={lr}, depth={md} -> R¬≤={r2:.4f}")
            if r2 > best[0]:
                best = (r2, {"n_estimators":ne,"learning_rate":lr,"max_depth_base":md}, pipe)

print("\n[AdaBoost] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

# @title RandomForestRegressor ‚Äî boucle manuelle + R¬≤(test)
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "n_estimators": [300, 600],
    "max_depth":    [12, 20, None],
    "max_features": ["sqrt", 0.7],
    "min_samples_leaf": [1, 3],
}
def ncomb(g):
    m=1
    for v in g.values(): m*=len(v)
    return m
print("Nombre de combinaisons test√©es:", ncomb(grid))

best = (-1e9, None, None)
for ne in grid["n_estimators"]:
    for md in grid["max_depth"]:
        for mf in grid["max_features"]:
            for msl in grid["min_samples_leaf"]:
                params = dict(
                    n_estimators=ne, max_depth=md, max_features=mf,
                    min_samples_leaf=msl, n_jobs=-1, random_state=0
                )
                pipe = Pipeline([
                    ("preprocess", preprocessor_v3_plus),
                    ("model", RandomForestRegressor(**params))
                ])
                pipe.fit(X_train, y_train)
                preds = np.clip(pipe.predict(X_test), 0, 100)
                r2 = r2_score(y_test, preds)
                print(f"RF n={ne}, depth={md}, max_feat={mf}, leaf={msl} -> R¬≤={r2:.4f}")
                if r2 > best[0]:
                    best = (r2, params, pipe)

print("\n[RF] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

# @title GradientBoostingRegressor ‚Äî grille compacte + R¬≤(test)
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "learning_rate": [0.05, 0.10],
    "n_estimators":  [800, 1200],
    "max_depth":     [2, 3],
    "subsample":     [1.0, 0.8]
}
print("Nombre de combinaisons test√©es:", len(grid["learning_rate"])*len(grid["n_estimators"])*len(grid["max_depth"])*len(grid["subsample"]))

best = (-1e9, None, None)
for lr in grid["learning_rate"]:
    for ne in grid["n_estimators"]:
        for md in grid["max_depth"]:
            for ss in grid["subsample"]:
                params = dict(learning_rate=lr, n_estimators=ne, max_depth=md, subsample=ss, random_state=0)
                pipe = Pipeline([
                    ("preprocess", preprocessor_v3_plus),
                    ("model", GradientBoostingRegressor(**params))
                ])
                pipe.fit(X_train, y_train)
                preds = np.clip(pipe.predict(X_test), 0, 100)
                r2 = r2_score(y_test, preds)
                print(f"GB lr={lr}, n={ne}, depth={md}, subsample={ss} -> R¬≤={r2:.4f}")
                if r2 > best[0]:
                    best = (r2, params, pipe)

print("\n[GB] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

# @title LightGBM (LGBMRegressor) ‚Äî mini-sweep stable + R¬≤(test)
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "learning_rate":     [0.05, 0.10],
    "n_estimators":      [1200, 1600],
    "num_leaves":        [96, 128, 160],
    "max_depth":         [12],
    "feature_fraction":  [0.7, 0.8],
    "bagging_fraction":  [0.8, 0.9],
    "bagging_freq":      [1],
    "min_child_samples": [10, 20],
    "reg_lambda":        [0.0, 0.5, 1.0],
}
def ncomb(g):
    m=1
    for v in g.values(): m*=len(v)
    return m
print("Nombre de combinaisons test√©es:", ncomb(grid))

best = (-1e9, None, None)
for lr in grid["learning_rate"]:
    for ne in grid["n_estimators"]:
        for nl in grid["num_leaves"]:
            for ff in grid["feature_fraction"]:
                for bf in grid["bagging_fraction"]:
                    for mcs in grid["min_child_samples"]:
                        for rl in grid["reg_lambda"]:
                            params = dict(
                                learning_rate=lr, n_estimators=ne, num_leaves=nl,
                                max_depth=12, feature_fraction=ff,
                                bagging_fraction=bf, bagging_freq=1,
                                min_child_samples=mcs, reg_lambda=rl,
                                objective="regression", random_state=0, n_jobs=-1
                            )
                            pipe = Pipeline([
                                ("preprocess", preprocessor_v3_plus),
                                ("model", LGBMRegressor(**params))
                            ])
                            pipe.fit(X_train, y_train)
                            preds = np.clip(pipe.predict(X_test), 0, 100)
                            r2 = r2_score(y_test, preds)
                            print(f"LGBM lr={lr}, n={ne}, leaves={nl}, ff={ff}, bf={bf}, mcs={mcs}, rl={rl} -> R¬≤={r2:.4f}")
                            if r2 > best[0]:
                                best = (r2, params, pipe)

print("\n[LGBM] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

pip install catboost

# @title CatBoostRegressor ‚Äî grille raisonnable + R¬≤(test)
from catboost import CatBoostRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

grid = {
    "learning_rate": [0.05, 0.10],
    "depth":         [8, 10, 12],
    "n_estimators":  [1200, 1600],
    "l2_leaf_reg":   [3, 5],
    "bagging_temperature": [0.25, 0.5],
}
print("Nombre de combinaisons test√©es:", len(grid["learning_rate"])*len(grid["depth"])*len(grid["n_estimators"])*len(grid["l2_leaf_reg"])*len(grid["bagging_temperature"]))

best = (-1e9, None, None)
for lr in grid["learning_rate"]:
    for dp in grid["depth"]:
        for ne in grid["n_estimators"]:
            for l2 in grid["l2_leaf_reg"]:
                for bt in grid["bagging_temperature"]:
                    params = dict(
                        learning_rate=lr, depth=dp, n_estimators=ne,
                        l2_leaf_reg=l2, bagging_temperature=bt,
                        loss_function="RMSE", random_state=0, verbose=False
                    )
                    # NB: preprocessor_v3_plus One-Hot d√©j√† ‚Üí on passe CatBoost en "num√©rique"
                    pipe = Pipeline([
                        ("preprocess", preprocessor_v3_plus),
                        ("model", CatBoostRegressor(**params))
                    ])
                    pipe.fit(X_train, y_train)
                    preds = np.clip(pipe.predict(X_test), 0, 100)
                    r2 = r2_score(y_test, preds)
                    print(f"CAT lr={lr}, depth={dp}, n={ne}, l2={l2}, bt={bt} -> R¬≤={r2:.4f}")
                    if r2 > best[0]:
                        best = (r2, params, pipe)

print("\n[CatBoost] Best params:", best[1], "| R¬≤(test)=", f"{best[0]:.4f}")

"""# Visualisation importances variables XGBoost"""

# @title XGBoost ‚Äî entra√Ænement final pour visualisations
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np

# Param√®tres finaux
xgb_params_final = dict(
    learning_rate=0.032, max_depth=12, n_estimators=3000,
    subsample=0.8, colsample_bytree=0.65,
    min_child_weight=1, reg_lambda=0.9, reg_alpha=0.05,
    tree_method='hist', random_state=seed, n_jobs=-1
)

# Pipeline complet
pipe_xgb_final = Pipeline([
    ("preprocess", preprocessor_v3_plus),
    ("model", XGBRegressor(**xgb_params_final))
])

# Entra√Ænement
pipe_xgb_final.fit(X_train, y_train)
y_pred_final = np.clip(pipe_xgb_final.predict(X_test), 0, 100)
r2_final = r2_score(y_test, y_pred_final)
print(f"[XGB final] R¬≤(test) = {r2_final:.4f}")

# ‚úÖ pr√™t pour plots : importances, PDP, calibration, etc.
single_pipe = pipe_xgb_final
single_hold = y_pred_final

# @title Importances XGBoost (top 30) avec noms de features r√©els
import numpy as np, matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

# Choisir le pipeline entra√Æn√©
xgb_pipe = single_pipe  # <- si tu utilises pipe_lossguide_best, remplace ici

# R√©cup√©ration des noms de features du pr√©processeur
def get_feature_names(preprocessor, X_df):
    names = []
    for name, trans, cols in preprocessor.transformers_:
        if name == 'remainder':
            # For 'remainder', use the column names directly
            remaining_cols = [col for col in X_df.columns if col not in preprocessor.transformers_[0][2] + preprocessor.transformers_[1][2] + preprocessor.transformers_[2][2] + preprocessor.transformers_[3][2] + preprocessor.transformers_[4][2]]
            names.extend(remaining_cols)
        elif isinstance(trans, Pipeline):
            # If it's a pipeline, iterate through steps
            for step_name, step_trans in trans.steps:
                if hasattr(step_trans, 'get_feature_names_out'):
                     names.extend(step_trans.get_feature_names_out(cols))
                elif isinstance(step_trans, FunctionTransformer):
                    # Handle FunctionTransformer by using original column names
                    names.extend(cols)
                else:
                    # Fallback for other transformers in a pipeline
                    names.extend(cols)
        elif hasattr(trans, 'get_feature_names_out'):
            # If the transformer has get_feature_names_out, use it
            names.extend(trans.get_feature_names_out(cols))
        elif isinstance(trans, FunctionTransformer):
             # Handle FunctionTransformer in the top level
             names.extend(cols)
        else:
            # Fallback for other transformers
            names.extend(cols if isinstance(cols, list) else list(cols))
    return np.array(names)


feat_names = get_feature_names(xgb_pipe.named_steps['preprocess'], X_train)
booster = xgb_pipe.named_steps['model']
imp = booster.feature_importances_

order = np.argsort(imp)[::-1][:30]
plt.figure(figsize=(8,10))
plt.barh(range(len(order)), imp[order][::-1])
plt.yticks(range(len(order)), feat_names[order][::-1])
plt.title("XGBoost ‚Äî Top 30 importances (gain)")
plt.xlabel("Importance")
plt.tight_layout(); plt.show()